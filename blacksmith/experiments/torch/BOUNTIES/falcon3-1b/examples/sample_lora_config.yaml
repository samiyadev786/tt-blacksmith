# SPDX-FileCopyrightText: (c) 2025 Tenstorrent AI ULC
#
# SPDX-License-Identifier: Apache-2.0
#
# Sample LoRA Configuration for Falcon3-1B
# ========================================
#
# This file demonstrates various LoRA configurations you can use
# for fine-tuning Falcon3-1B-Base.

# ============================================================
# Configuration 1: Low-Rank (Memory Efficient)
# ============================================================
# Use when memory is constrained or for quick experiments
#
# lora_r: 8
# lora_alpha: 16
# lora_dropout: 0.1
# lora_target_modules:
#   - "q_proj"
#   - "v_proj"
#
# Trainable params: ~0.2%
# Memory: ~3GB GPU/TT

# ============================================================
# Configuration 2: Medium-Rank (Balanced)
# ============================================================
# Good balance between quality and efficiency
#
# lora_r: 16
# lora_alpha: 32
# lora_dropout: 0.05
# lora_target_modules:
#   - "q_proj"
#   - "k_proj"
#   - "v_proj"
#   - "o_proj"
#
# Trainable params: ~0.4%
# Memory: ~4GB GPU/TT

# ============================================================
# Configuration 3: High-Rank (Best Quality)
# ============================================================
# Maximum expressiveness, requires more memory
#
# lora_r: 64
# lora_alpha: 128
# lora_dropout: 0.05
# lora_target_modules:
#   - "q_proj"
#   - "k_proj"
#   - "v_proj"
#   - "o_proj"
#   - "gate_proj"
#   - "up_proj"
#   - "down_proj"
#
# Trainable params: ~2%
# Memory: ~6GB GPU/TT

# ============================================================
# Recommended Configuration for TT-N150 (Default)
# ============================================================

model_name: "tiiuae/Falcon3-1B-Base"
training_type: "lora"

# LoRA hyperparameters
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_bias: "none"
lora_task_type: "CAUSAL_LM"

# Target all attention and MLP projections
lora_target_modules:
  - "q_proj"    # Query projection
  - "k_proj"    # Key projection
  - "v_proj"    # Value projection
  - "o_proj"    # Output projection
  - "gate_proj" # MLP gate
  - "up_proj"   # MLP up projection
  - "down_proj" # MLP down projection

# Training settings
learning_rate: 1e-4
batch_size: 4
gradient_accumulation_steps: 4
num_epochs: 3
warmup_steps: 100
weight_decay: 0.01

# Sequence length
max_length: 256

# Data type
dtype: "torch.bfloat16"
