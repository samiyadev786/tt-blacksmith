# SPDX-FileCopyrightText: (c) 2025 Tenstorrent AI ULC
#
# SPDX-License-Identifier: Apache-2.0
#
# Falcon3-1B-Base LoRA Training Configuration for TT-N150
# ========================================================
#
# This configuration is optimized for single-chip TT-N150 training.
# Run with: python train_falcon3_lora.py --config configs/tt_n150.yaml
#
# To verify TT execution, set LOGGER_LEVEL=DEBUG and look for TTIR graphs.

# Dataset settings
dataset_id: "wikitext-2"
dataset_name: "wikitext-2-raw-v1"

# Model settings
model_name: "tiiuae/Falcon3-1B-Base"
max_length: 256
dtype: "torch.bfloat16"

# Training hyperparameters
training_type: "lora"
learning_rate: 1e-4
batch_size: 4
gradient_accumulation_steps: 4
gradient_checkpointing: false
weight_decay: 0.01
num_epochs: 3
max_steps: -1  # Set to positive value to limit training steps
warmup_steps: 100
optim: "adamw_torch"
ignored_index: -100

# LoRA configuration
# Using moderate rank for balance between performance and memory
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"
lora_task_type: "CAUSAL_LM"
lora_bias: "none"

# Logging settings
log_level: "INFO"
use_wandb: true
wandb_project: "falcon3-1b-lora-tt-n150"
wandb_run_name: "falcon3-1b-lora-tt-n150"
wandb_tags:
  - "falcon3"
  - "lora"
  - "wikitext-2"
  - "tt-n150"
wandb_watch_mode: "all"
wandb_log_freq: 100
model_to_wandb: false
steps_freq: 10
epoch_freq: 1

# Checkpoint settings
resume_from_checkpoint: false
resume_option: "last"
checkpoint_path: ""
checkpoint_metric: "eval/loss"
checkpoint_metric_mode: "min"
keep_last_n: 3
keep_best_n: 2
save_strategy: "step"
project_dir: "blacksmith/experiments/torch/BOUNTIES/falcon3-1b"
save_optim: true
storage_backend: "local"
sync_to_storage: false
load_from_storage: false
remote_path: ""

# Reproducibility settings
seed: 42
deterministic: true

# Device settings - TT-N150 single chip
use_tt: true
# mesh_shape and mesh_axis_names are null for single-chip

# Fallback settings
# Enable fallback to handle any operations not yet supported on TT-N150
enable_fallback: true
fallback_operations: []
log_fallback_operations: true

# Validation settings
do_validation: true
eval_steps: 50
eval_batch_size: 4

# Other settings
output_dir: "results/falcon3-1b-tt-n150"
logging_steps: 10
do_train: true
print_examples: true
framework: "pytorch"
save_plots: true
plots_dir: "plots"

# Perplexity tracking
compute_perplexity: true
