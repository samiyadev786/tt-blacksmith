# Dataset settings
dataset_id: "mnist"
train_ratio: 0.8
dtype: "torch.bfloat16"

# Model settings
model_name: "MNISTLinear"
input_size: 784
hidden_size: 512
output_size: 10
bias: false

# Training hyperparameters
learning_rate: 0.01
batch_size: 256
num_epochs: 16
train_log_steps: 100
val_log_epochs: 5
loss_fn: "torch.nn.MSELoss"
optim: "sgd"

# Reproducibility settings
seed: 23
deterministic: false

# Logging settings
log_level: "INFO"
use_wandb: true
wandb_project: "blacksmith-mnist-tp"
wandb_run_name: "mnist_multichip"
wandb_tags: ["tt-xla", "model:torch", "plugin", "wandb"]
wandb_watch_mode: "all"
wandb_log_freq: 100
model_to_wandb: false
steps_freq: 100
epoch_freq: 5

# Checkpoint settings
resume_from_checkpoint: false
resume_option: "last"        # [last, best, path]
checkpoint_path: ""          # path to checkpoint if resume_option is "path"
checkpoint_metric: "val/loss"
checkpoint_metric_mode: "min" # [min, max]
keep_last_n: 3
keep_best_n: 1
save_strategy: "epoch"
project_dir: "blacksmith/experiments/torch/mnist"
save_optim: false
storage_backend: "local"
sync_to_storage: false
load_from_storage: false
remote_path: ""

# Device settings - Tensor Parallel with 2 devices
mesh_shape: [1, 2]  # [data, model]
mesh_axis_names: ["data", "model"]

# Model sharding patterns using regex (matches module names from model.named_modules())
model_sharding_patterns:
    - ["linear_relu_stack\\.0$", [null, "model"]]
    - ["linear_relu_stack\\.2$", ["model", null]]
    - ["linear_relu_stack\\.4$", [null, "model"]]

# Other settings
device: "TT"
experiment_name: "torch-mnist-tp"
framework: "pytorch"
output_dir: "experiments/results/mnist"
use_tt: true
print_examples: false
